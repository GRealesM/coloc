<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline1">1. A brief outline of colocalisation analysis</a></li>
<li><a href="#orgheadline2">2. Usage</a></li>
<li><a href="#orgheadline6">3. Proportional testing</a>
<ul>
<li><a href="#orgheadline3">3.1. Principal components</a></li>
<li><a href="#orgheadline4">3.2. Bayesian model averaging</a></li>
<li><a href="#orgheadline5">3.3. Using Bayes Factors to compare specific values of \(\eta\)</a></li>
</ul>
</li>
<li><a href="#orgheadline9">4. (Approximate) Bayes Factor colocalisation analyses</a>
<ul>
<li><a href="#orgheadline7">4.1. Introduction</a></li>
<li><a href="#orgheadline8">4.2. The basic <code>coloc.abf</code> function</a></li>
</ul>
</li>
<li><a href="#orgheadline10">5. The difference between proportional and ABF approaches</a></li>
</ul>
</div>
</div>

<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{Colocalisation vignette}
-->

# A brief outline of colocalisation analysis<a id="orgheadline1"></a>

<font color="grey">
*Chris Wallace // [web](http://chr1swallace.github.io) // [email](mailto:cew54 at cam.ac.uk)*  
</font>

The coloc package can be used to perform genetic colocalisation
analysis of two potentially related phenotypes, to ask whether they
share common genetic causal variant(s) in a given region.  There are a
few key references which this vignette will not duplicate (see below),
but, in brief, two approaches are implemented.

First, the proportional approach uses the fact that for two traits
sharing causal variants, regression coefficients for either trait
against any set of SNPs in the neighbourhood of those variants must be
proportional.  This test was first proposed by Plagnol et al. <sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
in the context of evaluating whether expression of the gene *RPS26*
mediated the association of type 1 diabetes to a region on chromosome
12q13 as had recently been proposed.  The test addressed a common
issue in genetics, and meant researchers could avoid the need to
squint at parallel manhattan plots to decide whether two traits share
causal variants.  The function `coloc.test()` in this package evolved
from code released by Vincent, but no longer available.

However, choosing **which** SNPs to use for the test is a problem.
The obvious choice is to use those most strongly associated with one
or other trait to maximise information, but doing so induces bias in
the regression coefficients, which in turn leads to increased
likelihood of spuriously rejecting the null of colocalisation, ie
a quite substantially increased type 1 error rate <sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>.  I proposed
two alternatives to address this problem, either using a principal
component summary of genetic variation in the region to overcome the
need to select a small set of test SNPs, implemented in `coloc.pcs()`
and associated functions, or to use the ideas of Bayesian model
averaging to average p values over SNP selections, generating
posterior predictive p values, implemented in `coloc.bma()`.

Proportional testing, however, requires individual level genotype
data, which are not always available.  Claudia Giambartolomei and
Vincent Plagnol have proposed an alternative method, which makes use
of Jon Wakefield's work on determining approximate Bayes Factors from
p values <sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup> to generate a Bayesian colocalisation analysis <sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>,
implemented in the function `coloc.summaries()`.  Note that it is
possible to use Bayesian analysis for proportional testing too, in
determining the posterior distribution of the proportionality
constant \(\eta\).

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Suitability of functions in this package for different data structures</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Function</th>
<th scope="col" class="org-left">Approach</th>
<th scope="col" class="org-left">Data required</th>
<th scope="col" class="org-left">Dependence</th>
</tr>


<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">between datasets</th>
</tr>
</thead>

<tbody>
<tr>
<td class="org-left">coloc.abf</td>
<td class="org-left">Bayesian</td>
<td class="org-left">summary statistics</td>
<td class="org-left">independent</td>
</tr>


<tr>
<td class="org-left">coloc.abf.datasets</td>
<td class="org-left">Bayesian</td>
<td class="org-left">raw genotypes</td>
<td class="org-left">independent</td>
</tr>


<tr>
<td class="org-left">coloc.bma</td>
<td class="org-left">Proportional, averages</td>
<td class="org-left">raw genotypes</td>
<td class="org-left">independent</td>
</tr>


<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">over models</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>


<tr>
<td class="org-left">coloc.pcs/coloc.test</td>
<td class="org-left">Proportional, based on</td>
<td class="org-left">raw genotypes</td>
<td class="org-left">independent</td>
</tr>


<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">PC summary of genotypes</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

# Usage<a id="orgheadline2"></a>

Let's simulate a small dataset, and compare the three methods.

```{r echo=FALSE}
library(coloc)
setClass("simdata",
	 representation(df1="data.frame",df2="data.frame"))
setValidity("simdata", function(object) {
  n <- nrow(object@df1)
  if(nrow(object@df2)!=n)
    return("nrow of '@df1' should equal nrow of '@df2'")
})
setMethod("show", signature="simdata", function(object) {
  cat("pair of simulated datasets, with",ncol(object@df1)-1,"SNPs and",nrow(object@df1),"samples.\n")
})

sim.data <- function(nsnps=50,nsamples=200,causals=1:2,nsim=1) {
  cat("Generate",nsim,"small sets of data\n")
  ntotal <- nsnps * nsamples * nsim
  X1 <- matrix(rbinom(ntotal,1,0.4)+rbinom(ntotal,1,0.4),ncol=nsnps)
  Y1 <- rnorm(nsamples,rowSums(X1[,causals]),2)
  X2 <- matrix(rbinom(ntotal,1,0.4)+rbinom(ntotal,1,0.4),ncol=nsnps)
  Y2 <- rnorm(nsamples,rowSums(X2[,causals]),2)
  colnames(X1) <- colnames(X2) <- paste("s",1:nsnps,sep="")
  df1 <- cbind(Y=Y1,X1)
  df2 <- cbind(Y=Y2,X2)
  if(nsim==1) {
    return(new("simdata",
	       df1=as.data.frame(df1),
	       df2=as.data.frame(df2)))
  } else {
    index <- split(1:(nsamples * nsim), rep(1:nsim, nsamples))
    objects <- lapply(index, function(i) new("simdata", df1=as.data.frame(df1[i,]),
					     df2=as.data.frame(df2[i,])))
    return(objects)
  }
}

## simulate some data and load the coloc library
set.seed(46411)
data <- sim.data(nsamples=1000,nsim=1)
library(coloc)
```

If you are not familiar with the analysis of genetic data in R, I
would suggest beginning with the [snpStats package](http://www.bioconductor.org/packages/release/bioc/html/snpStats.html), which has
functions for reading data in a variety of formats and contains a
vignette on data input.  Once you have your genetic data loaded as a
SnpMatrix, `X`, and phenotype information in a vector `Y`, then
conversion to a `data.frame` as used above follows

    df <- as.data.frame(cbind(Y=Y, as(X,"numeric")))

# Proportional testing<a id="orgheadline6"></a>

## Principal components<a id="orgheadline3"></a>

The code below first prepares a principal component object by combining
the genotypes in the two dataset, then models the most informative
components (the minimum set required to capture 80% of the genetic
variation) in each dataset, before finally testing whether there is
colocalisation between these models.

```{r fig=TRUE}
## run a coloc with pcs
pcs <- pcs.prepare(data@df1[,-1], data@df2[,-1])
pcs.1 <- pcs.model(pcs, group=1, Y=data@df1[,1], threshold=0.8)
pcs.2 <- pcs.model(pcs, group=2, Y=data@df2[,1], threshold=0.8)
ct.pcs <- coloc.test(pcs.1,pcs.2)
```

The plot shows the estimated coefficients for each principal component
modeled for traits 1 and 2 on the x and y axes, with circles showing
the 95% confidence region.  The points lie close to the line through
the origin, which supports a hypothesis of colocalisation.

A little more information is stored in the `ct.pcs` object:

```{r }
ct.pcs
str(summary(ct.pcs))
```

The best estimate for the coefficient of proportionality,
\(\hat{\eta}\), is 1.13, and the null hypothesis of colocalisation is
not rejected with a chisquare statistic of 5.27 based on 7 degrees of
freedom (\(n-1\) where the \(n\) is the number of components tested, and
one degree of freedom was used in estimating \(\eta\)), giving a p value
of 0.63.  The `summary()` method returns a named vector of length 4
containing this information.

If more information is needed about \(\eta\), then this is available if
the `bayes` argument is supplied:

```{r }
ct.pcs.bayes <- coloc.test(pcs.1,pcs.2, bayes=TRUE)
ci(ct.pcs.bayes)
```

## Bayesian model averaging<a id="orgheadline4"></a>

This approach appears simpler.  There is no need to do any
preparatory work, you require only a single function:

```{r fig=TRUE}
ct.bma <- coloc.bma(data@df1, data@df2, 
		    family1="gaussian", family2="gaussian",
		    plot.coeff=TRUE)
ct.bma.bayes <- coloc.bma(data@df1, data@df2, 
			  family1="gaussian", family2="gaussian", 
			  bayes=TRUE)
```

The `family1/2` parameters are used to specify the model and link
function, ="gaussian"= (with identity link) or ="binomial"= (with
logit link) are accepted values.  The plot here shows all models
considered simultaneously, and does not attempt to discriminate
between models with strong and weak support.  It's rather confusing,
and is off by default.

However, `coloc.bma()` is doing
quite some work to cover the model space efficiently, and it is
important to understand how it does this.  First, the `r2.trim`
parameter is used to "tag" the SNPs - a subset of SNPs are selected so
that no pair have \(r^2>\) `r2.trim`.  The default value is 0.95 and the
idea is that models containing SNPs with very similar genotypes
provide little additional information, so the \(p\) value need be
averaged over only one of each such group.  Lower values of `r2.trim`
will produce a sparser model space and so decrease computation.
Second, the `thr` parameter is used to discard SNPs which are
uninformative with regards the phenotype, that is, if \(pp_{ij}\) is the
posterior probability of inclusion in single SNP models for SNP \(i\),
trait \(j\), the set of discarded SNPs is formed by those for which
\(pp_{i1}<\) `thr` and \(pp_{i2}<\) `thr`.  Models containing **only** SNPs
from this set will be ignored.  Note that models containing one SNP
from this set and one SNP *not* in the set **will** be evaluated.

Finally, you should tell `coloc.bma()` how many SNPs should be
included in each model.  The default is `nsnps=2`, 3 appears slightly
more powerful but will generally require considerably more
computation, whilst values of 4 and above are both unlikely to
provide more information and very unlikely to be computed in any
reasonable time for interactive work.

## Using Bayes Factors to compare specific values of \(\eta\)<a id="orgheadline5"></a>

It may be that specific values of \(\eta\) are of interest.  For
example, when comparing eQTLs in two tissues, or when comparing risk
of two related diseases, the value \(\eta=1\) is of particular
interest.  In proportional testing, we can use Bayes Factors to
compare the support for different values of \(\eta\).  Eg

```{r }
## compare individual values of eta
ct.pcs <- coloc.test(pcs.1,pcs.2, bayes.factor=c(-1,0,1))
bf(ct.pcs)

## compare ranges of eta
ct.bma <- coloc.bma(data@df1, data@df2, 
		    family1="gaussian", family2="gaussian",
		    bayes.factor=list(c(-0.1,1), c(0.9,1.1)))
bf(ct.bma)
```

# (Approximate) Bayes Factor colocalisation analyses<a id="orgheadline9"></a>

## Introduction<a id="orgheadline7"></a>

The idea behind the ABF analysis is that the association of
each trait with SNPs in a region may be summarised by a vector of 0s
and at most a single 1, with the 1 indicating the causal SNP (so,
assuming a single causal SNP for each trait).  The posterior
probability of each possible configuration can be calculated and so,
crucially, can the posterior probabilities that the traits share
their configurations.  This allows us to estimate the support for the
following cases:

-   \(H_0\): neither trait has a genetic association in the region
-   \(H_1\): only trait 1 has a genetic association in the region
-   \(H_2\): only trait 2 has a genetic association in the region
-   \(H_3\): both traits are associated, but with different causal variants
-   \(H_4\): both traits are associated and share a single causal variant

## The basic `coloc.abf` function<a id="orgheadline8"></a>

The function `coloc.abf` is ideally suited to the case when only
summary data are available, and requires, for each trait, either:

-   p values for each SNP
-   each SNP's minor allele frequency
-   sample size
-   ratio of cases:controls (if using a case-control trait)

or:

-   regression coefficients for each SNP
-   variance of these regression coefficients.

If regression coefficients and their variance are available, please
use these, but we can also approximate Bayes Factors from p values and
minor allele frequencies, although, note, such approximation can be
less accurate when imputed data are used.  NB, you can mix and match,
depending on the data available from each study.

A wrapper function is avalailable to run all these steps but we will
first generate the p values manually to give all the details. We use the snpStats library from
Bioconductor to calculate the p values quickly.  We will also calculate the coefficients and their standard errors to compare the two approaches (but you should use only one - the coefficients if available, otherwise the p values).

```{r }
if(requireNamespace("snpStats")) {
library(snpStats)

Y1 <- data@df1$Y
Y2 <- data@df2$Y

X1 <- new("SnpMatrix",as.matrix(data@df1[,-1]))
X2 <- new("SnpMatrix",as.matrix(data@df2[,-1]))

p1 <- snpStats::p.value(single.snp.tests(phenotype=Y1, snp.data=X1),df=1)
p2 <- snpStats::p.value(single.snp.tests(phenotype=Y2, snp.data=X2),df=1)

maf <- col.summary(X2)[,"MAF"]

get.beta <- function(...) {
   tmp <- snpStats::snp.rhs.estimates(..., family="gaussian")
   beta <- sapply(tmp,"[[","beta")
   varbeta <- sapply(tmp, "[[", "Var.beta")
   return(list(beta=beta,varbeta=varbeta))
}
b1 <- get.beta(Y1 ~ 1, snp.data=X1)
b2 <- get.beta(Y2 ~ 1, snp.data=X2)
} else {
Y1 <- data@df1$Y
Y2 <- data@df2$Y

X1 <- as.matrix(data@df1[,-1])
X2 <- as.matrix(data@df2[,-1])

tests1 <- lapply(1:ncol(X1), function(i) summary(lm(Y1 ~ X1[,i]))$coefficients[2,])
tests2 <- lapply(1:ncol(X2), function(i) summary(lm(Y2 ~ X2[,i]))$coefficients[2,])

p1 <- sapply(tests1,"[",4)
p2 <- sapply(tests2,"[",4)

maf <- colMeans(X2)/2

get.beta <- function(x) {
   beta <- sapply(x,"[",1)
   varbeta <- sapply(x, "[", 2)^2
   return(list(beta=beta,varbeta=varbeta))
}
b1 <- get.beta(tests1)
b2 <- get.beta(tests2)
}
```

Note that we are using the second dataset in that case to compute the minor allele frequencies.
This is unlikely to make any significant difference but one could have used dataset 1 instead.
It is now possible to compute the probabilities of interest.

With coefficients, and std. deviation of Y known, we have the most accurate inference.  Note that if you have a case-control trait, set type="cc" and supply s, the proportion of samples in the dataset that are cases (from which sdY can be derived), instead of giving sdY directly.

```{r }
my.res <- coloc.abf(dataset1=list(beta=b1$beta, varbeta=b1$varbeta, N=nrow(X1),sdY=sd(Y1),type="quant"),
		    dataset2=list(beta=b2$beta, varbeta=b2$varbeta, N=nrow(X2),sdY=sd(Y2),type="quant"),
		    MAF=maf)
print(my.res[[1]])
```

When std. deviation of Y is unknown, coloc will try and estimate sdY from variance of beta and MAF, but this can be noisy, particularly for small datasets, and will produce a warning:

```{r }
my.res <- coloc.abf(dataset1=list(beta=b1$beta, varbeta=b1$varbeta, N=nrow(X1),type="quant"),
		    dataset2=list(beta=b2$beta, varbeta=b2$varbeta, N=nrow(X2),type="quant"),
		    MAF=maf)
print(my.res[[1]])
```

When only p values are available, we can use the form:

```{r }
my.res <- coloc.abf(dataset1=list(pvalues=p1,N=nrow(X1),type="quant"),
		    dataset2=list(pvalues=p2,N=nrow(X2),type="quant"),
		    MAF=maf)
print(my.res[[1]])
```

Here, as we have simulated full genotype data, we can use
the wrapper function `coloc.abf.datasets()` to combine all the steps
shown above.

```{r }
ct.abf <- coloc.abf.datasets(data@df1, data@df2, response1="Y", response2="Y",
			     type1="quant", type2="quant")
```

# The difference between proportional and ABF approaches<a id="orgheadline10"></a>

So what are the differences between proportional and ABF approaches?
Which should you choose?

Well, if you only have p values, then you must use ABF.  But be aware
that a single causal variant is assumed, and that for accurate
inference, the causal variant needs to be included, so either very
dense or imputed genotyping data is needed.  The ABF approach has
another big advantage over proportional testing: being Bayesian, it
allows you to evaluate support for each hypothesis, whereas with
proportional testing, when the null of colocalisation is not rejected,
you cannot be sure whether this reflects true colocalisation or a lack
of power.  The proportional approach is much less affected by
spareness of genotyping data, with power only slightly decreased, but
type 1 error rates unaffected.

The behaviour of the two approaches only really differs when there are
two or more causal variants.  When both are shared, proportional
testing has the same type 1 error rate, and whilst ABF tends to still
favour \(H_4\) (shared variant), it tends to put some more weight on
\(H_3\) (distinct variants).  But when at least one is specific to one
trait and at least one is shared, their behaviour differs more
substantially.  Of course, neither approach was designed with this
case in mind, and so there is no "right" answer, but it is instructive
to understand their expected behaviour.  Proportional testing tends to
reject colocalisation, whilst ABF tends to favour sharing.  Until the
methods are extended to incorporate this specific case, it can be
useful to compare the two approaches when complete and dense
genotyping data are available.  When the results differ, we have
tended to identify a combination of shared and distinct causal
variants.  The ABF approach can still be applied in this case, if p
values are available conditioning on the strongest signal, as
demonstrated in our paper <sup><a id="fnr.4.100" class="footref" href="#fn.4">4</a></sup>.

You can see more about the ABF approach on
[this blogpost](http://haldanessieve.org/2013/05/21/our-paper-bayesian-test-for-co-localisation-between-pairs-of-genetic-association-studies-using-summary-statistics/).

<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><http://www.ncbi.nlm.nih.gov/pubmed/19039033></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><http://onlinelibrary.wiley.com/doi/10.1002/gepi.21765/abstract></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><http://www.ncbi.nlm.nih.gov/pubmed/18642345></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004383></div></div>


</div>
</div>
